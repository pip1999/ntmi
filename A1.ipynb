{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**\n",
    "\n",
    "* [Language models](#lm)\n",
    "* [Unigram LMs](#unigram)\n",
    "* [Smoothing](#smooth)\n",
    "* [Higher-Order LMs](#higher)\n",
    "* [Interpolation](#inter)\n",
    "* [Evaluation](#eval)  \n",
    "* [Tuning hyperparameters](#hparams)\n",
    "    \n",
    "**Table of Exercises**\n",
    "\n",
    "* [Exercise 1](#ex1) (-/3)\n",
    "* [Exercise 2](#ex2) (-/2)\n",
    "* [Exercise 3](#ex3) (-/1)\n",
    "* [Exercise 4](#ex4) (-/1)\n",
    "* [Exercise 5](#ex5) (-/3)\n",
    "\n",
    "**General notes**\n",
    "\n",
    "* In this notebook you are expected to use $\\LaTeX$. \n",
    "* Use python3\n",
    "\n",
    "**ILOs**\n",
    "\n",
    "After completing this lab you should be able to \n",
    "\n",
    "* develop ngram language models\n",
    "* estimate parameters of LMs via MLE\n",
    "* evaluate LMs intrinsically in terms of perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"lm\"> Language Models\n",
    "    \n",
    "As we began discussing in [T1](../T1/T1.ipynb), a language model (LM) is a **probability distribution over text**, where text is a finite sequences of words. \n",
    "    \n",
    "LMs can be used to generate text as well as to quantify a degree of naturalness (or rather, a degree of resemblance to training data) which is useful for example to compare and rank alternative pieces of text in terms of fluency.\n",
    "    \n",
    "To design an LM, we need to talk about units of text (e.g., documents, paragraphs, sentences) as outcomes of a random experiment, and for that we need *random variables*. If you need a recap on probability theory, don't miss out [our refresher](../R1/R1.ipynb).\n",
    "    \n",
    "We will generally refer to the unit of text as a *sentence*, but that's just for convenience, you could design an LM over documents and very little, if anything, would change.  \n",
    "\n",
    "A **random sentence** is a finite sequence of tokens in the vocabulary of a given language. As a running example, we will refer to this language as English. The vocabulary of English will be denoted by $\\Sigma$, a finite collection of unique tokens (which, as we discussed in T1, may include words, punctuation, special symbols, etc). We will denote a random sentence by $S$, or more explicitly, by the random sequence $X_{1:m} = \\langle X_1, \\ldots, X_m \\rangle$. Here $m$ indicates the sequence length. Each token in the sequence is a random variable that takes on values in $\\mathcal X$, an enumeration of $\\Sigma$. We will adopt an important convention, every sentence is a finite sequence that ends with a special symbol, the end-of-sequence (EOS) symbol. Clearly, random sentences take on values in the set $\\Sigma^*$ of all strings made of symbols in $\\Sigma$, which is a set that does include an infinte number of valid English sentences (possibly not all English sentences, as our vocabulary may not be complete enough, but hopefully this space is still large enough for the LM to be useful) as well as an infinite number of sequences that are not valid English sentences.       \n",
    "    \n",
    "Part of the deal with a language model is to define and estimate a probability distribution that expresses a preference for sentences that are more likely to be accepted as English sentences. In practice an LM will prefer sentences that reproduce statistics of the observations used to estimate its parameters, whether these sentences will resemble English sentences or not will depend on how expressive the LM is, that is, whether or not the LM can capture patterns as complex as those arising from well-formed English (or whatever variant/register of English was observed during training).\n",
    "    \n",
    "**Notation guide** Some textbooks or papers use $X_1^m$ instead of $X_{1:m}$ for ordered sequences, both are clear enough, but we will use the notation adopted by the textbook, that is, $X_{1:m}$. The textbook uses $X_1\\cdots X_m$ (without commas) as another notation for ordered sequences, but we prefer to explicitly mark the sequence with angle brackets to avoid ambiguities, i.e., we prefer $\\langle X_1, \\ldots, X_m \\rangle$. The textbook is not very formal with respect to probability distribution notation, we prefer to be more formal about it, so we will subscript $P$ with the random variable(s) of interest, e.g., $P_{X_1:m}$ for a distribution over sequences, or $P_X$ for a distribution over words, or $P_{X|H}$ for a conditional distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz 1** Define a categorical random variable $X$ for words sampled from a closed vocabulary $\\Sigma$ (assume the size of the vocabulary is denoted by $v$). In your answer make sure you indicate what is the sample space and the precise support $\\mathcal X$ of the categorical random variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>SOLUTION</b></summary>\n",
    "\n",
    "Let $X$ be a random variable that maps from a vocabulary $\\Sigma$ to an index set of $\\Sigma$ (the support) via an enumeration. The sample space is $\\Sigma$, that is, the words in the language. The support of the random variable is the enumeration, that is the set of integers from 1 to $v$, i.e. $\\mathcal X = \\{1, \\ldots, v\\}$.\n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notational shortcuts are rather convenient:\n",
    "\n",
    "* we will often use $X_{1:m}$ for a random sentence instead of the longer form $\\langle X_1, \\ldots, X_m \\rangle$, and similarly for outcomes (i.e., $x_{1:m}$ instead of $\\langle x_1, \\ldots, x_m\\rangle$), but in the long form we shall never drop the angle brackets, as otherwise it's hard to tell that we mean an ordered sequence\n",
    "* we will use $X_{<i}$ (or $x_{<i}$ for an outcome) to denote the sequence of tokens that precedes the $i$th token, this sequence is empty $X_{<i} \\triangleq \\langle \\rangle$ for $i \\le 1$, for $i>1$ the sequence is defined as $X_{<i} \\triangleq \\langle X_1, \\ldots, X_{i-1}\\rangle$\n",
    "* sometimes it will be useful to find a more compact notation for $X_{<i}$, in those cases we refer to it as a *random history* and denote it by $H_i$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LMs can be described by a **generative story**, that is, a stochastic procedure that explains how an outcome $x_{1:m}$ is drawn from the model distribution. Though we may find inspiration in how we believe the data were generated, the generative story is not a faithful representation of any linguistic process, it is all but an abstraction that codes our own assumptions about the problem. \n",
    "\n",
    "The most general form this generative story can take, that is, the form with the least amount of assumptions, looks as follows:\n",
    "\n",
    "1. For each position $i$ of a sequence, condition on the history $h_i$ and draw the $i$th word from the conditional probability distribution $P_{X|H=h_i}$. \n",
    "2. Stop generating if $X_i$ is the EOS token    \n",
    "\n",
    "We say this procedure is very general because it is essentially just chain rule spelled out in English words, though here the order of enumeration is determined by the left-to-right order of tokens in an English sentence.\n",
    "\n",
    "\n",
    "Here is an example for a sequence of length $m=3$:\n",
    "\n",
    "$P_S(\\langle x_1, x_2, x_3 \\rangle) = P_{X|H}(x_1|\\langle \\rangle) P_{X|H}(x_2|\\langle x_1 \\rangle) P_{X|H}(x_3 | \\langle x_1, x_2 \\rangle)$\n",
    "\n",
    "For our example sentence *He went to the store* this means:\n",
    "\n",
    "\\begin{align}\n",
    "P_S(\\langle \\text{He, went, to, the, store, EOS} \\rangle) &= P_{X|H}(\\text{He}|\\langle \\rangle) \\\\\n",
    "    &\\times P_{X|H}(\\text{went}|\\langle \\text{He} \\rangle) \\\\\n",
    "    &\\times P_{X|H}(\\text{to}|\\langle \\text{He}, \\text{went} \\rangle) \\\\\n",
    "    &\\times P_{X|H}(\\text{the}|\\langle \\text{He},  \\text{went}, \\text{to} \\rangle) \\\\\n",
    "    &\\times P_{X|H}(\\text{store}|\\langle \\text{He},  \\text{went}, \\text{to}, \\text{the} \\rangle) \\\\\n",
    "    &\\times P_{X|H}(\\text{EOS}|\\langle \\text{He},  \\text{went}, \\text{to}, \\text{the}, \\text{store} \\rangle) \n",
    "\\end{align}\n",
    "\n",
    "* where with some abuse of notation we use the words themselves as outcomes instead of their corresponding indices. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz 2**  Write down the general rule for the probability $P_S$ of a sentence $x_{1:m}$. Don't forget to use subscripts to indicate the precise random variable associated with every distribution (that is, for example, $P_S$ is correct while $P$ is wrong). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>SOLUTION</b></summary>\n",
    "\n",
    "$P_S(x_{1:m}) = \\prod_{i=1}^{m}P_{X|H}(x_i|x_{<i})$\n",
    "\n",
    "where $m$ is the length of the sentence.\n",
    "    \n",
    "</details>\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LM above is just an abstraction, not a concrete implementation, think of it as a general template for building models. \n",
    "\n",
    "A concrete model design needs to specify the conditional probability distributions in the model, this is known as the **parameterisation** of the model, and an algorithm for parameter estimation. \n",
    "\n",
    "In T1 we briefly discussed the unigram language model, we will revisit it, extend it to deal with unknown words, and then relax some of its strong independence assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"unigram\"> Unigram LM\n",
    "\n",
    "\n",
    "We start with the simplest language model: the idea is to forget the history completely, therefore making the strong assumption that words are drawn from the same distribution independently of thei preceding context (i.e., $X_i \\perp X_{<i}$):\n",
    "\n",
    "\\begin{equation}\n",
    "P_S(x_{1:m}) \\overset{\\text{ind.}}{\\triangleq}  \\prod_{i=1}^m P_X(x_i)\n",
    "\\end{equation}\n",
    "\n",
    "For the parameterisation, we let $X$ follow a Categorical distribution with parameter $\\theta_{1:v} \\in \\Delta^{v-1}$. \n",
    "\n",
    "Thus, the final <a name=\"eq-unigram-lm\">unigram LM definition</a> is \n",
    "\n",
    "\\begin{equation}\n",
    "P_S(x_{1:m}|\\theta_{1:v}) \\triangleq \\prod_{i=1}^m \\text{Cat}(X=x_i|\\theta_{1:v}) \n",
    "\\end{equation}\n",
    "\n",
    "If you need a recap of probability theory, check [this notebook](../R1/R1.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz 3**  Complete the categorical probability mass function (pmf) and the conditions below:\n",
    "\n",
    "$\\text{Cat}(X=a|\\theta_{1:v}) = $ *type the pfm*\n",
    "\n",
    "where $\\theta_{1:v} = \\langle \\theta_1, \\ldots, \\theta_v\\rangle$ are the categorical parameters for which it must hold\n",
    "\n",
    "1. *type condition 1*\n",
    "2. *type condition 2*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>SOLUTION</b></summary>\n",
    "\n",
    "$\\text{Cat}(X=a|\\theta_{1:v}) = \\prod_{i=1}^v \\theta_x^{\\delta_{xa}}$\n",
    "\n",
    "or \n",
    "\n",
    "$\\text{Cat}(X=a|\\theta_{1:v}) = \\prod_{i=1}^v \\theta_x^{[x=a]}$\n",
    "\n",
    "\n",
    "or \n",
    "\n",
    "$\\text{Cat}(X=a|\\theta_{1:v}) = \\theta_a$\n",
    "\n",
    "where $\\theta_1^v$ are the categorical parameters for which it must hold\n",
    "\n",
    "\n",
    "1. $0 < \\theta_x < 1$ for $x \\in [1, v]$\n",
    "2. $\\sum_{x=1}^v \\theta_x = 1$ \n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Example***\n",
    "\n",
    "Consider the sentence *He went to the store*, its probability under the unigram LM is\n",
    "\n",
    "$P_S(\\langle \\text{He, went, to, the, store, EOS} \\rangle) = P_X(\\text{He}) \\times P_X(\\text{went}) \\times P_X(\\text{to}) \\times P_X(\\text{the}) \\times P_X(\\text{store}) \\times P_X(\\text{EOS})$\n",
    "\n",
    "which, as a function of the parameters of the model, evaluates to\n",
    "\n",
    "$P_S(\\langle \\text{He, went, to, the, store, EOS} \\rangle) = \\theta_{\\text{He}} \\times \\theta_{\\text{went}} \\times \\theta_{\\text{to}} \\times \\theta_{\\text{the}} \\times \\theta_{\\text{store}} \\times \\theta_{\\text{EOS}}$\n",
    "\n",
    "where again we use the words instead of their indices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a concrete instance of a language model:\n",
    "* a set of independence assumptions (the unigram assumption)\n",
    "* a parameterisation (we will use a single Categorical distribution)\n",
    "\n",
    "If we are given the parameters $\\theta_{1:v}$, we will be able to sample $S$ from this model, and we will be able to assess the probability the model assigns to any given sentence in its sample space. \n",
    "\n",
    "So, we just need to find a way to choose $\\theta_{1:v}$. We could pick any vector in the probability simplex. Clearly, some probability vectors are better than others though, so we better find a procedure that enjoys some theoretical support.\n",
    "\n",
    "We can turn to basic statistics for help, in particular, we can turn to maximum likelihood estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum likelihood estimation\n",
    "\n",
    "Suppose we are given a corpus $\\mathcal D$ containing $K$ sentences\n",
    "\n",
    "* each sentence is of the form $x_{1:m_k}^{(k)}$ for $k=1, \\ldots, K$\n",
    "* where $m_k$ is the length of the $k$th sentence\n",
    "\n",
    "The MLE solution for the unigram LM is based on gathering counts and computing the relative frequency of word types:\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_x = \\frac{\\mathrm{count}_X(x)}{\\sum_{x' \\in \\mathcal X}\\mathrm{count}_X(x')} =  \\frac{\\overbrace{\\sum_{k=1}^K \\sum_{i=1}^{m_k} [x^{(k)}_i = x]}^{\\text{count }X=x\\text{ in }\\mathcal D}}{\\text{number of tokens in }\\mathcal D}\n",
    "\\end{equation}\n",
    "\n",
    "Note that the *number of tokens* is simply the sum of the length of the sentences $\\sum_{k=1}^K m_k$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Let's quickly implement the unigram LM (this implementation will differ slightly from the one we developed in T1, this way you get to appreciate some diversity of designs).\n",
    "\n",
    "Let us use the PTB dataset (the NLTK version which is already segmented into sentences and tokens, [see T1](../T1/T1.ipynb)). We will estimate the categorical parameters and query the LM with some sentences to find out their probability.\n",
    "\n",
    "Notes: \n",
    "\n",
    "1. Use python dict to store the parameters of our unigram distribution.\n",
    "2. For English, we lowercase the data to collect better statistics (otherwise 'He' and 'he' would correspond to different words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank as ptb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we start by **loading and pre-processing data**. NLTK's PTB is already tokenized, so we only do case folding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_ptb = [[w.lower() for w in s] for s in ptb.sents()]\n",
    "print(\"The PTB has {} sentences, here are the first few:\".format(len(prep_ptb)))\n",
    "for i, s in zip(range(5), prep_ptb):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to pick an EOS symbol, as that symbol will be part of the unigram distribution. \n",
    "For convenient we will also pick a begin-of-sentence (BOS) symbol, wich will be useful later or when we look inot higher-order LMs. \n",
    "\n",
    "Let's first verify that our choices of BOS and EOS are not already in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "ptb_original_vocab = set(chain(*prep_ptb))  # itertools.chain is very handy, check its documentation if you don't already know it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"<s>\" not in ptb_original_vocab, \"Our choice of BOS symbol should not clash with an existing token\"\n",
    "assert \"</s>\" not in ptb_original_vocab, \"Our choice of EOS symbol should not clash with an existing token\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we approach statistical learning, it is convenient to hold some data out of training. These data will help us adjust some model decisions throughout development and compare alternatives at the end. \n",
    "\n",
    "We will typically split the available data in 3 disjoint sets:\n",
    "\n",
    "* *training* data: the portion we used for parameter estimation via MLE\n",
    "* *development* data: the portion we used in the development phase to adjust some of our design choices\n",
    "* *test* data: the portion that is kept heldout from training and development until we have decided on as few competing systems as possible, we use this portion to appreciate our model's potential for generalisation to novel data\n",
    "\n",
    "An alternative to this fixed split, which is generally more robust, is *cross-validation*, however, for many NLP systems cross-validation adds a considerable amount of computation, so in our labs we will not be using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_corpus(sentences, ratio=0.8):\n",
    "    \"\"\"\n",
    "    Randomly split a list of sentences into two sets according to the given ratio.\n",
    "    \n",
    "    Though the split is random, we fix the random seed of the random number generator, \n",
    "     this is good for reproducibility, it will allows us to compare your run to our runs.\n",
    "    \n",
    "    :param sentences: already tokenized sentences (list of strings)\n",
    "    :returns: portion containing (ratio)*100% of the data, portion containing (1-ratio)*100% of the data\n",
    "    \"\"\"\n",
    "    # This will guarantee that the permutation is the same every time (which is important for reproducibility)\n",
    "    rng = np.random.RandomState(42)\n",
    "    rng.permutation(5)\n",
    "    indices = rng.permutation(len(sentences))\n",
    "    n = int(indices.size * ratio)\n",
    "    return [sentences[i] for i in indices[:n]], [sentences[i] for i in indices[n:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use 80% for training, 10% for development, and 10% for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_training, prep_test = split_corpus(prep_ptb)\n",
    "prep_dev, prep_test = split_corpus(prep_test, ratio=0.5)\n",
    "\n",
    "print(\"Number of observations: training={} development={} test={}\".format(len(prep_training), len(prep_dev), len(prep_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's **count unigrams**, much like we did in T1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def count_unigrams(sentences, EOS=\"</s>\"):\n",
    "    \"\"\"\n",
    "    input: a generator of preprocessed sentences\n",
    "        - a preprocessed sentence is a list of lowercased tokens\n",
    "    output: \n",
    "        unigram_count: dictionary of frequency of each word\n",
    "    \"\"\"    \n",
    "    unigram_counts = Counter()\n",
    "    for sentence in sentences:\n",
    "        unigram_counts.update(sentence + [EOS])\n",
    "    \n",
    "    ### Here is an alternative (less compact) way of counting unigrams\n",
    "    #unigram_counts = defaultdict(int)\n",
    "    #for sentence in sentence_stream:\n",
    "    #    for token in sentence:\n",
    "    #        unigram_counts[token] += 1  # frequency of each word\n",
    "    \n",
    "    return unigram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_count_table =  count_unigrams(prep_training)\n",
    "assert unigram_count_table['</s>'] == len(prep_training), \"EOS should occur as many times as there are sentences in the corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test our procedure and check how many times 'cat', 'mat', and '</s>' happen in the PTB training corpus\n",
    "print('unigram=cat count={}'.format(unigram_count_table['cat']))\n",
    "print('unigram=mat count={}'.format(unigram_count_table['mat']))\n",
    "print('unigram=</s> count={}'.format(unigram_count_table['</s>']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is MLE as we saw in T1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def unigram_mle(unigram_counts: Counter):\n",
    "    \"\"\"\n",
    "    input: unigram_count: dictionary of frequency of each word\n",
    "           \n",
    "    output: unigram_prob: dictionary with the probabilty of each word \n",
    "            (parameters of the model)\n",
    "    \"\"\"\n",
    "    \n",
    "    total_count = sum(unigram_counts.values())\n",
    "    unigram_probs = dict()\n",
    "    \n",
    "    for word, count in unigram_counts.items():\n",
    "        unigram_probs[word] = float(count) / total_count            \n",
    "        \n",
    "    return unigram_probs\n",
    "\n",
    "# Let's check the MLE parameters associated with 'cat' and 'mat' by querying their unigram probabilities\n",
    "unigram_prob_table = unigram_mle(unigram_count_table)\n",
    "assert all(0 <= p <= 1. for p in unigram_prob_table.values()), \"Probabilities are between 0 and 1\"\n",
    "assert np.isclose(sum(unigram_prob_table.values()), 1., 0.0001), \"The coordinates of a probability vector add up to 1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is how we check the probability of a token, in a way that unknown words won't lead to a KeyError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('unigram=cat prob=%f' % unigram_prob_table.get('cat', 0.0))  # 0.0 is the default, returned in case the key is not in the dict\n",
    "print('unigram=mat prob=%f' % unigram_prob_table.get('mat', 0.0))\n",
    "print('unigram=ntmi prob=%f' % unigram_prob_table.get('ntmi', 0.0))\n",
    "print('unigram=</s> prob=%f' % unigram_prob_table.get('</s>', 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert unigram_prob_table.get('ntmi', 0.0) == 0.0, \"NLMI has not made it to the PTB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look how interesting: the probability of the EOS token is very close to the inverse of the average sequence length in the corpus\n",
    "assert np.isclose(1./unigram_prob_table.get('</s>', 0.0), np.mean([len(s) for s in prep_training]), 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz 4** Why is it that $1/\\theta_{\\text{EOS}}$ is essentially equal to the average sequence length in the corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>SOLUTION</b></summary>\n",
    "\n",
    "The MLE estimate for EOS is\n",
    "\n",
    "\\begin{align}\n",
    "\\theta_{\\text{EOS}} &= \\frac{\\mathrm{count}_X(\\text{EOS})}{\\text{number of tokens in corpus}}\n",
    "\\end{align}\n",
    "\n",
    "the numerator is number of sentences (we appended EOS to the end of each and every sequence in the corpus), and `number of tokens in corpus` is the sum of sequence lengths, while `average sequence length` is precisely `sum of sequence lengths` divided by `number of sequences`.\n",
    "    \n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz 5** Compute the log-probability of a sentence under the unigram LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_sentence_unigram_log_probability(sentence, word_probs, EOS=\"</s>\", UNK=\"<unk>\"):\n",
    "    \"\"\"\n",
    "    input: list of words in a sentence\n",
    "    word_probs: MLE paremeters\n",
    "    output:\n",
    "            sentence_probability_sum: log probability of the sentence\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>SOLUTION</b></summary>\n",
    "\n",
    "```python\n",
    "    \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_sentence_unigram_log_probability(sentence, word_probs, EOS=\"</s>\", UNK=\"<unk>\"):\n",
    "    \"\"\"\n",
    "    input: list of words in a sentence\n",
    "    word_probs: MLE paremeters\n",
    "    output:\n",
    "            sentence_probability_sum: log probability of the sentence\n",
    "    \"\"\"\n",
    "    sentence_log_probability = 0.\n",
    "    # we first get the probability of unknown words\n",
    "    #  which by default is 0. in case '<unk>' is not in the support\n",
    "    unk_probability = word_probs.get(UNK, 0.)\n",
    "    for word in sentence + [EOS]:\n",
    "        # this will return `unk_probability` if the word is not in the support\n",
    "        word_probability = word_probs.get(word, unk_probability)  \n",
    "        # it is a sum of log pboabilities\n",
    "        # we use np.log because it knows that log(0) is float('-inf')\n",
    "        sentence_log_probability += np.log(word_probability)\n",
    "    return sentence_log_probability  \n",
    "    \n",
    "```\n",
    "    \n",
    "</details>    \n",
    "    \n",
    "---    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_sentence_unigram_log_probability(['the', 'cat', 'sat', 'on', 'the', 'cat'], unigram_prob_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(\n",
    "    calculate_sentence_unigram_log_probability(['the', 'cat', 'sat', 'on', 'the', 'cat'], unigram_prob_table), \n",
    "    -46.05, 0.01), \"Unless you are not using the PTB or pre-processed it differently, I expected about -46.05\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can assess the logarithm of the joint probability of subsets of our training data, for example, the first 10 sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(calculate_sentence_unigram_log_probability(s, unigram_prob_table) for i, s in zip(range(10), prep_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(\n",
    "    sum(calculate_sentence_unigram_log_probability(s, unigram_prob_table) for i, s in zip(range(10), prep_training)), \n",
    "    -2110.23, 0.1), \"Unless you are not using the PTB or pre-processed it differently, I expected about -2110.23\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unseen words\n",
    "\n",
    "However, note that if we want the probability of sentences containing words that are not present in the training corpus, we will have an unpleasant surprise. \n",
    "\n",
    "For example: *NLMI is fun*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_sentence_unigram_log_probability(['ntmi', 'is', 'fun'], unigram_prob_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heldout data will almost surely contain unseen words (remember Zipf's law?):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(calculate_sentence_unigram_log_probability(s, unigram_prob_table) for i, s in zip(range(10), prep_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz 6** Why did we get $-\\infty$ above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>SOLUTION</b></summary>\n",
    "\n",
    "If a token is unknown it contributes a $0$ probability to the product in the joint probability $P_{X_{1:m}}(x_{1:m}|\\boldsymbol \\theta)$, which then becomes $0$, whose logarithm is $-\\infty$.\n",
    "\n",
    "Of course that would not happen if we had smoothed our language model.\n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"smooth\"> Smoothing\n",
    "\n",
    "Note that MLE will fail if we evaluate on sentences containing words that the model has never seen (at training). \n",
    "    \n",
    "The words we haven't seen before are called unknown words, or **out of vocabulary** (OOV) words.\n",
    "We will now map them to a special UNK symbol such as `<unk>`.\n",
    "    \n",
    "To keep the LM from assigning zero probability to these unseen words, weâ€™ll have to steal some of the probability mass from some more frequent words and give it to words we've never seen.\n",
    "This is called **smoothing** or **discounting**.\n",
    "\n",
    "The simplest form of smoothing is called **Laplace smoothing**, whereby we add `<unk>` to the support of the distribution and then add one to all counts before we normalize them into probabilities. \n",
    "All the counts that used to be zero will now have a count of 1, the counts of 1 will be 2, and so on. \n",
    "\n",
    "We can also generalise it and add $\\alpha > 0$ instead of $1$. Then for $X \\sim \\text{Cat}(\\theta_{1:v})$ we get the MLE solution:\n",
    "\n",
    "\\begin{align}\n",
    "\\theta_x &= \\frac{ \\mathrm{count}_X(x) + \\alpha}{\\sum_{x' \\in \\mathcal X} (\\mathrm{count}_X(x') + \\alpha)} \\\\\n",
    "    &= \\frac{ \\mathrm{count}_X(x) + \\alpha}{\\alpha v + \\sum_{x' \\in \\mathcal X} \\mathrm{count}_X(x')}\n",
    "\\end{align}\n",
    "    \n",
    "    \n",
    "There are $v$ words in the vocabulary and each one was incremented by $\\alpha$, we also need to adjust the denominator to take into account the extra $\\alpha v$ observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz 7** Implement the smoothed MLE procedure for the unigram distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_smoothed_mle(unigram_counts: Counter, alpha=1.0, UNK=\"<unk>\"):\n",
    "    \"\"\"\n",
    "    input: unigram_count: dictionary of frequency of each word\n",
    "           \n",
    "    output: unigram_prob: dictionary with the (smoothed) probabilty of each word \n",
    "            (parameters of the model)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>SOLUTION</b></summary>\n",
    "    \n",
    "```python\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "def unigram_smoothed_mle(unigram_counts: Counter, alpha=1.0, UNK=\"<unk>\"):\n",
    "    \"\"\"\n",
    "    input: unigram_count: dictionary of frequency of each word\n",
    "           \n",
    "    output: unigram_prob: dictionary with the probabilty of each word \n",
    "            (parameters of the model)\n",
    "    \"\"\"\n",
    "    assert UNK not in unigram_counts, \"Choose an UNK symbol that is not currently part of the vocabulary\"\n",
    "    \n",
    "    unigram_probs = dict()\n",
    "    \n",
    "    total_count = sum(unigram_counts.values())\n",
    "    vocab_size = len(unigram_counts) + 1  # + 1 as we intend to consider UNK as part of the vocabulary\n",
    "    denominator = total_count + alpha * vocab_size \n",
    "    \n",
    "    for word, count in chain([(UNK, 0)], unigram_counts.items()):\n",
    "        unigram_probs[word] = (float(count) + alpha) / denominator    \n",
    "        \n",
    "    return unigram_probs\n",
    "```\n",
    "    \n",
    "</details>    \n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's smooth the unigram distribution and appreciate the effect on training data as well as heldout data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the MLE parameters associated with 'cat' and 'mat' by querying their unigram probabilities\n",
    "unigram_smoothed_prob_table = unigram_smoothed_mle(unigram_count_table)\n",
    "assert all(0 <= p <= 1. for p in unigram_prob_table.values()), \"Probabilities are between 0 and 1\"\n",
    "assert np.isclose(sum(unigram_smoothed_prob_table.values()), 1., 0.0001), \"The coordinates of a probability vector add up to 1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert unigram_smoothed_prob_table.get('ntmi', unigram_smoothed_prob_table[\"<unk>\"]) > 0.0, \"Smoothing!\"\n",
    "assert unigram_smoothed_prob_table.get('ntmi', unigram_smoothed_prob_table[\"<unk>\"]) == unigram_smoothed_prob_table.get('mat', unigram_smoothed_prob_table[\"<unk>\"]), \"Smoothing!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob_first10_before = sum(calculate_sentence_unigram_log_probability(s, unigram_prob_table) for i, s in zip(range(10), prep_training))\n",
    "log_prob_first10_after  = sum(calculate_sentence_unigram_log_probability(s, unigram_smoothed_prob_table) for i, s in zip(range(10), prep_training))\n",
    "assert log_prob_first10_after > log_prob_first10_before, \"Smoothing generalise improves the probability of observed sentences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert - np.infty < sum(calculate_sentence_unigram_log_probability(s, unigram_smoothed_prob_table) for i, s in zip(range(10), prep_dev)), \"Smoothing should assign non-zero probability to heldout data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"higher\"> Higher order LMs\n",
    "\n",
    "    \n",
    "A unigram LM makes some rather unreasonable assumptions. Clearly, words in a sentence do depend on one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz 8** Can the unigram LM assign different probabilities to `what a nice day` and `day what nice a`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>SOLUTION</b></summary>\n",
    "    \n",
    "    No really. The sentence contains the exact same tokens and they occur the exact same number of times.\n",
    "    \n",
    "    \n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to capture dependencies in English sentences, we better go back to the general chain rule\n",
    "    \n",
    "\\begin{equation}\n",
    "P_S(x_{1:m}) = \\prod_{i=1}^{m}P_{X|H}(x_i|x_{<i})\n",
    "\\end{equation}\n",
    "\n",
    "and ask ourselves, why did we make such a strong independence assumption in the unigram LM and can we avoid it?\n",
    "\n",
    "\n",
    "The deal is that with our current way of parameterising a Categorical distribution, we need to store  a $v$-dimensional parameter vector for every unique history in the training data. This type of parameterisation, where we have one parameter (a probability value) per outcome per context, is known as **tabular** (you can imagine storing the parameters in a table, each row is a context, each column is an outcome).\n",
    "The more words we allow in the context, the more parameters we will have to estimate, and this tabular representation grows very quickly. Not only this costs a lot of parameters, most histories will appear very few times, a very long history will probably only appear once. Thus we will not be able to gather enough data to estimate the parameters of the LM reliably. \n",
    "\n",
    "We have at least two ways around this problem: we can revisit the model from the point of view of the (conditional) independencies we make, or we can change the way we parameterise Categorical distributions to make them more parameter efficient. This week we are going to do the former. Next week we begin to learn more about the latter. The combination of the two strategies lies at the core of much of the progress in modern NLP.\n",
    "\n",
    "The core of the problem is the data sparsity due to long histories, then let's shorten the histry. But unlike the unigra LM, we won't discard the history entirely, we will just forget some of it instead. \n",
    "\n",
    "The $n$-gram LM assumes that $X_i$ is independent of all but the $n-1$ immediately preceding words. It uses Categorical distributions to model the distribution of $X_i | H_i=x_{i-n+1:i-1}$ in context. \n",
    "\n",
    "**Notation guide** As $x_{i-n+1:i-1}$ is a bit cumbersome to write, we will write $x_{[<i]_n}$ to denote the $i$th prefix shortened to $n-1$ words.\n",
    "\n",
    "\n",
    "In its standard formulation, the $n$-gram LM uses tabular cpds, that is, it stores one Categorical distribution per unique history: $X | H=h \\sim \\text{Cat}(\\theta_{1:v}^{(h)})$ with $\\theta_{1:v}^{(h)} \\in \\Delta^{v-1}$ for each history $h$ (a sequence of $n-1$ tokens).\n",
    "\n",
    "The joint probability the $n$-gram LM assigns to a sentence $x_{1:m}$ is\n",
    "\n",
    "\\begin{align}\n",
    "P_S(x_{1:m}|\\boldsymbol \\theta) &\\overset{cond.ind.}{\\triangleq} \\prod_{i=1}^m P_{X|H}(x_i|\\underbrace{x_{[<i]_n}}_{=h_i}) \\\\\n",
    "    &= \\prod_{i=1}^m \\text{Cat}(x_i|\\theta_{1:v}^{(h_i)}) \n",
    "\\end{align}\n",
    "\n",
    "**Notation guide** We are using $\\boldsymbol \\theta$ to denote the collection of all prameters of all cpds.\n",
    "\n",
    "The $n$-gram LM is what we call a **Markov model** of order $o=n-1$, the order indicates the length of the shortened history we condition on when drawing $X_i$.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz 9**  Write down the probability of the sentence \n",
    "\n",
    "    He went to the store\n",
    "    \n",
    "under a bigram language model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>SOLUTION</b></summary>\n",
    "\n",
    "$P_S(\\langle \\text{He, went, to, the, store, EOS} \\rangle) = P_X(\\text{He}) \\times P_X(\\text{went}|\\langle \\text{He} \\rangle) \\times P_X(\\text{to}| \\langle \\text{went} \\rangle) \\times P_X(\\text{the}| \\langle \\text{to} \\rangle) \\times P_X(\\text{store}|\\langle \\text{the} \\rangle) \\times P_X(\\text{EOS}|\\langle \\text{store} \\rangle)$\n",
    "\n",
    "Tip: recall that *the* is a word while $\\langle \\text{the} \\rangle$ is a sequence, and recall that though sentences in corpora don't usually come decorated with an EOS, our LMs treat them as if they did.\n",
    "    \n",
    "Trick: it's quite handy to imagine that we had $n-1$ occurences of a begin-of-sentence (BOS) symbol prior to the first token in the sentence, this gives us a fixed-size history across time steps: the first step would look like $P_X(\\text{He}|\\langle BOS \\rangle)$.\n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter estimation\n",
    "\n",
    "The Laplace-smoothed MLE solution for tabular cpds is simply\n",
    "\n",
    "\\begin{equation}\n",
    "(4) \\qquad \\theta_x^{(h)} = \\frac{\\mathrm{count}_{HX}(h \\circ \\langle x \\rangle) + \\alpha}{\\mathrm{count}_H(h) + \\alpha v}\n",
    "\\end{equation}\n",
    "\n",
    "where  $h \\circ \\langle x \\rangle$ is the concatenation of history and word. \n",
    "    \n",
    "    \n",
    "**Notation guideline** When we use a superscript like $\\theta_{1:v}^{(h)}$ we mean the probability vector selected by the history, or the probability vector that is specific to the cpd that conditions on the given history $h$. For example, in a bigram LM, we have one $v$-dimensional parameter vector for each one of the $v$ cpds in the model. That is, there is one cpd per word in our vocabulary, because each time we condition on a history $\\langle w \\rangle$, we get a different cpd, and each of these cpds is a discrete distribution over the entire vocabulary. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz 10**  What is the space complexity of an $n$-gram LM? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>SOLUTION</b></summary>\n",
    "\n",
    "The $n$-gram LM requires a collection of cpds, each cpd takes the form $\\text{Cat}(\\theta_{1:v}^{(h)})$ where $h$ has size $n-1$ by definition. As $h$ is made of symbols in the vocabulary of the language, the maximum number of unique histories we can have is therefore $v^{n-1}$. This means we have an upperbound on the number of cpds. Each cpd is a discrete distribution over the vocabulary, that is, each $\\theta_{1:v}^{(h)} \\in \\Delta^{v-1}$ is a $v$-dimensional probability vector. \n",
    "    \n",
    "The space complexity of a general $n$-gram LM is therefore $\\mathcal O(v^n)$.\n",
    "    \n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz 11**  What is the time complexity to assess the joint probability of a sentence under an $n$-gram LM? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>SOLUTION</b></summary>\n",
    "\n",
    "To assess the joint probability of a sentence $x_{1:m}$, we need to scan over each position $i$ from $1$ to $m$ and look up the probability of $x_i$ in context. Assuming that lookup operations are efficient, i.e. they run in constant time $\\mathcal O(1)$, the upperbound on runtime is linear in sentence length $\\mathcal O(m)$.\n",
    "    \n",
    "    \n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex1\" style=\"color:red\">**Exercise 1**</a> In this exercise you will build a general $n$-gram language model where $n \\ge 1$. We provide you with a skeleton class on which to build. \n",
    "\n",
    "\n",
    "* Start by implementing the method `count_ngrams`, see the documentation of the method for specification. Tip: expand upon the procedure implemented in the function `count_unigrams` above; remember to handle BOS tokens and EOS tokens correctly. Use `<s>` for BOS token and `</s>` for EOS token.\n",
    "* Now implement the method `solve_mle`, see the documentation of the method for specification.\n",
    "* Finally, implement the `log_prob` method, see the documentation of the method for specification.\n",
    "\n",
    "You should fill in a table like the following one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "headers = ['training', 'heldout', 'order', 'alpha', 'perplexity']\n",
    "rows = [\n",
    "    ['ptb', 'ptb-dev', 0, [0], None],    # this is a unigram LM\n",
    "    ['ptb', 'ptb-dev', 0, [1],  None],    # this is a smoothed unigram LM\n",
    "    ['ptb', 'ptb-dev', 1, [1, 0],  None],  # this is a smoothed bigram LM (with smoothed unigram counts)\n",
    "    ['ptb', 'ptb-dev', 1, [1, 1],  None],  # this is a smoothed bigram LM (with smoothed unigram and bigram counts)\n",
    "]\n",
    "print(\"Log probability assigned to the sentences in the dev set\")\n",
    "print(tabulate(rows, headers=headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "class LM:\n",
    "    \"\"\"\n",
    "    This is an n-gram LM with generalised Laplace smoothing.    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, order, alpha=0.0, BOS=\"<s>\", EOS=\"</s>\", UNK=\"<unk>\"):  \n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self._order = order\n",
    "        self._count_table = dict()\n",
    "        self._prob_table = dict()\n",
    "        self._vocab = set()\n",
    "        self._BOS = BOS\n",
    "        self._EOS = EOS\n",
    "        self._UNK = UNK\n",
    "        self._alpha = alpha   \n",
    "        \n",
    "        # in Laplace smoothing we always add '<unk>' to the vocabulary\n",
    "        self._vocab.add(UNK)\n",
    "        \n",
    "    def order(self):\n",
    "        return self._order\n",
    "    \n",
    "    def vocab_size(self):\n",
    "        return len(self._vocab)\n",
    "             \n",
    "    def preprocess_history(self, history):\n",
    "        \"\"\"\n",
    "        This function pre-process an arbitrary history to match the order of this language model.\n",
    "        :param history: a sequence of words\n",
    "        :return: a tuple containing exactly as many elements as the order of the model\n",
    "            - if the input history is too short we pad it with <s> \n",
    "        \"\"\"\n",
    "        if len(history) == self._order:\n",
    "            history = tuple(history)\n",
    "        elif len(history) > self._order:\n",
    "            length = len(history)            \n",
    "            history = tuple(history[length - self._order: length])\n",
    "        else:  # here the history is too short\n",
    "            missing = self._order - len(history)\n",
    "            history = tuple([self._BOS] * missing) + tuple(history)\n",
    "        assert len(history) == self._order, \"Make sure the history has a fixed-length, this simplifies implementation\"\n",
    "        return history\n",
    "                \n",
    "    def get_parameter(self, history, word):\n",
    "        \"\"\"\n",
    "        This function returns the categorical parameter associated with a certain word given a certain history.\n",
    "        :param history: a sequence of words (a tuple)\n",
    "        :param word: a word (a str)\n",
    "        :return: a float representing P(word|history)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"I am an assignment, implement me!\")\n",
    "        \n",
    "    def cpd_items(self, history):\n",
    "        history = self.preprocess_history(history)\n",
    "        # if the history is unseen we return an empty cpd\n",
    "        return self._prob_table.get(history, dict()).items()\n",
    "        \n",
    "    def count_ngrams(self, data_stream):\n",
    "        \"\"\"\n",
    "        This function should populate the attribute _count_table which should be understood as \n",
    "            - a python dict \n",
    "                - whose key is a history (a tuple of words)\n",
    "                - and whose value is itself a python dict (or defaultdict)\n",
    "                    - which maps a word (a string) to a count (an integer)\n",
    "        \n",
    "        This function will add counts to whatever counts are already stored in _count_table.\n",
    "        \n",
    "        :param data_stream: a generator as produced by `preprocess`\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"I am an assignment, implement me!\")\n",
    "                    \n",
    "    def solve_mle(self):\n",
    "        \"\"\"\n",
    "        This function should compute the attribute _prob_table which has the exact same structure as _count_table\n",
    "         but stores probability values instead of counts. \n",
    "        It can be seen as the collection of cpds of our model, that is, _prob_table\n",
    "            - maps a history (a tuple of words) to a dict where\n",
    "                - a key is a word (that extends the history forming an ngram)\n",
    "                - and the value is the probability P(word|history)                \n",
    "                \n",
    "        This function will replace whatever value _prob_table currently stores by the newly computed MLE solution.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"I am an assignment, implement me!\")\n",
    "        \n",
    "    def log_prob(self, sentence):\n",
    "        \"\"\"\n",
    "        Compute the log probability of a sentence under this model. \n",
    "                \n",
    "        input: \n",
    "            sentence: a sequence of tokens\n",
    "        output:\n",
    "            log probability\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"I am an assignment, implement me!\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play a bit with a unigram LM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **DEBUG PTB**\n",
    "unigram_lm = LM(order=0)\n",
    "unigram_lm.count_ngrams(prep_training)\n",
    "unigram_lm.solve_mle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unigram_lm.log_prob(\"the new rate will be payable feb. 15 .\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(\n",
    "    unigram_lm.log_prob(\"the new rate will be payable feb. 15 .\".split()), \n",
    "    -65.36, 0.01), \"Unless you changed the corpus, the preprocessing, or the random seed, I expected -65.36\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, our model always supports the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, s in zip(range(5), prep_training):\n",
    "    print('{:.2f} {}'.format(unigram_lm.log_prob(s), ' '.join(s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the dev set may contain words/tokens unseen during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, s in zip(range(5), prep_dev):\n",
    "    print('{:.2f} {}'.format(unigram_lm.log_prob(s), ' '.join(s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smoothing will circumvent that problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **DEBUG PTB**\n",
    "smoothed_unigram_lm = LM(order=0, alpha=1.)\n",
    "smoothed_unigram_lm.count_ngrams(prep_training)\n",
    "smoothed_unigram_lm.solve_mle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not necessarily the case that a smoothed LM will improve the probability of already observed sentences (after all we are stealing probability from some $n$-grams and moving it around to unseen outcomes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i, s in zip(range(5), prep_training):\n",
    "    rows.append([i + 1, unigram_lm.log_prob(s), smoothed_unigram_lm.log_prob(s)])\n",
    "print('Sentence from training data')    \n",
    "print(tabulate(rows, headers=['i', 'unigram LM', 'smoothed unigram LM']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But smoothing will deal with the problem of 0 probabilities for unobserved sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i, s in zip(range(5), prep_dev):\n",
    "    rows.append([i + 1, unigram_lm.log_prob(s), smoothed_unigram_lm.log_prob(s)])\n",
    "print('Sentence from development data')    \n",
    "print(tabulate(rows, headers=['i', 'unigram LM', 'smoothed unigram LM']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the LM order will generally improve the probability of already observed sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **DEBUG PTB**\n",
    "bigram_lm = LM(order=1)\n",
    "bigram_lm.count_ngrams(prep_training)\n",
    "bigram_lm.solve_mle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i, s in zip(range(5), prep_training):\n",
    "    rows.append([i + 1, unigram_lm.log_prob(s), bigram_lm.log_prob(s)])\n",
    "    assert bigram_lm.log_prob(s) > unigram_lm.log_prob(s), \"Generally we expect the bigram LM to assign higher probabilities to training data than the unigram LM\"\n",
    "print('Sentence from training data')    \n",
    "print(tabulate(rows, headers=['i', 'unigram LM', 'bigram LM']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But generalising to novel data is even harder now: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i, s in zip(range(5), prep_dev):\n",
    "    rows.append([i + 1, unigram_lm.log_prob(s), bigram_lm.log_prob(s)])\n",
    "print('Sentence from development data')    \n",
    "print(tabulate(rows, headers=['i', 'unigram LM', 'bigram LM']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz 12** Why is it harder to generalise to novel data as the LM order increases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>SOLUTION</b></summary>\n",
    "   \n",
    "Due to data sparsity. Without any smoothing, we will return 0 probability to unseen bigrams, the space of possible bigrams is much larger (i.e., $\\{1, \\ldots, v\\} \\times \\{1, \\ldots, v\\}$) than the space of possible unigrams (i.e., $\\{1, ..., v\\}$) for the same amount of training data, thus it's more likely that we will encounter unseen bigrams after training.  \n",
    "   \n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smoothing overcomes 0s, but we will have other problems, as we shall see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **DEBUG PTB**\n",
    "smoothed_bigram_lm = LM(order=1, alpha=1.)\n",
    "smoothed_bigram_lm.count_ngrams(prep_training)\n",
    "smoothed_bigram_lm.solve_mle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i, s in zip(range(5), prep_dev):\n",
    "    rows.append([i + 1, bigram_lm.log_prob(s), smoothed_bigram_lm.log_prob(s)])\n",
    "print('Sentence from development data')    \n",
    "print(tabulate(rows, headers=['i', 'bigram LM', 'smoothed bigram LM']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz 13** If we compare a unigram LM (with Laplace smoothing) and a bigram LM (with Laplace smoothing for unigram and bigram distributions), we will see that smoothing is not doing a great job. While it does prevents $0$ probabilities for novel sentences, it sometimes makes the model a lot worse for already observed data. How would you explain this behaviour of smoothing applied to sparser distributions (such as the higher-order distributions)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i, s in zip(range(5), prep_training):\n",
    "    rows.append(['training {}'.format(i + 1), smoothed_unigram_lm.log_prob(s), smoothed_bigram_lm.log_prob(s)])\n",
    "for i, s in zip(range(5), prep_dev):\n",
    "    rows.append(['dev {}'.format(i + 1), smoothed_unigram_lm.log_prob(s), smoothed_bigram_lm.log_prob(s)])    \n",
    "print(tabulate(rows, headers=['i', 'smoothed unigram LM', 'smoothed bigram LM']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>SOLUTION</b></summary>\n",
    "\n",
    "Consider a conditional distribution $X|H=\\langle w \\rangle \\sim \\text{Cat}(\\theta_{1:v}^{\\langle w \\rangle})$ for some single-word history. The parameters of this cpdf are estimated using bigram counts. Bigrams are sparser than unigrams, perhaps given $\\langle w \\rangle$, we only have $5$ observations in the training data. This means that we have only $5$ actual counts, against $\\alpha v - 5$ fake counts (due to generalised Laplace smoothing). It's easy to see that unless $\\alpha$ is quite small, unobserved outcomes will accummulate far more probabability mass than observed outcomes. \n",
    "    \n",
    "To make smoothing works this constant $\\alpha$ needs to be chosen rather carefully. \n",
    "    \n",
    "</details>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our advice is that you smooth low-order LMs (e.g., unigram LMs, maybe bigram LMs), but use other techniques for higher-order models, as we show next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"inter\"> Interpolation\n",
    "\n",
    "Laplace smoothing deals with unseen words for a seen history, but it is not the best strategy for unseen histories. A simple idea is to use language model interpolation in order to obtain more robust statistics for our ngrams. \n",
    "\n",
    "We interpolate language models $\\mathcal M_0, \\ldots, \\mathcal M_o$, where $\\mathcal M_j$ is a Markov model of order $j$, to obtain an interpolated $(o+1)$-gram language model. For the interpolation we use coefficients $\\lambda_0, \\ldots, \\lambda_o$ where\n",
    "\n",
    "* $0 < \\lambda_j < 1$\n",
    "* $\\sum_{j=0}^{o} \\lambda_j = 1$\n",
    "\n",
    "The probability of a sentence $x_{1:m}$ under the <a name=\"inter-snt-prob\">interpolated model</a> is\n",
    "\n",
    "\\begin{equation}\n",
    "(8) \\qquad P_S(x_{1:m}|\\mathcal M_0, \\ldots, \\mathcal M_o) = \\prod_{i=1}^m P_{X|H}(x_i|x_{<i}, \\mathcal M_0, \\ldots, \\mathcal M_o)\n",
    "\\end{equation}\n",
    "\n",
    "where the <a name=\"inter-factor\">interpolated factor is </a>\n",
    "\n",
    "\\begin{equation}\n",
    "(9) \\qquad P_{X|H}(x_i|x_{<i}, \\mathcal M_0, \\ldots, \\mathcal M_{n-1}) = \\sum_{j=0}^{o} \\lambda_j \\times P_{X|H}(x_i|x_{i-j:i-1}, \\mathcal M_j)\n",
    "\\end{equation}\n",
    "\n",
    "and $ P_{X|H}(x|h, \\mathcal M_j)$ is the probability of the $(j+1)$-gram suffix of $h \\circ \\langle x \\rangle$ under a model of order $j$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For example, consider the sentence `here comes the sun`, for a $3$-gram LM (order $2$) we pad it `BOS BOS here comes the sun EOS` and compute interpolated factors:\n",
    "\n",
    "\\begin{align}\n",
    "P_{X|H}(\\text{here} \\mid \\langle \\text{BOS, BOS} \\rangle) &= \\lambda_0 \\times P_{X|H}(\\text{here} \\mid \\langle \\rangle, \\mathcal M_0) \\\\\n",
    "&+ \\lambda_1 P_{X|H}(\\text{here}\\mid \\langle \\text{BOS} \\rangle, \\mathcal M_1) \\\\\n",
    "&+ \\lambda_2 P_{X|H}(\\text{here} \\mid \\langle \\text{BOS, BOS} \\rangle, \\mathcal M_2) \\\\\n",
    "P_{X|H}(\\text{comes}\\mid \\langle \\text{BOS, here} \\rangle) &= \\lambda_0 \\times P_{X|H}(\\text{comes}\\mid \\langle \\rangle, \\mathcal M_0) \\\\\n",
    "&+ \\lambda_1 P_{X|H}(\\text{comes}\\mid\\langle \\text{here} \\rangle,  \\mathcal M_1) \\\\\n",
    "&+ \\lambda_2 P_{X|H}(\\text{comes}\\mid \\langle \\text{BOS, here} \\rangle,  \\mathcal M_2) \\\\\n",
    "P_{X|H}(\\text{the}\\mid \\langle \\text{here, comes} \\rangle) &= \\lambda_0 \\times P_{X|H}(\\text{the}\\mid\\langle \\rangle, \\mathcal M_0) \\\\\n",
    "&+ \\lambda_1 P_{X|H}(\\text{the}\\mid \\langle \\text{comes} \\rangle,  \\mathcal M_1) \\\\\n",
    "&+ \\lambda_2 P_{X|H}(\\text{the}\\mid\\langle \\text{here, comes} \\rangle,  \\mathcal M_2) \\\\\n",
    "P_{X|H}(\\text{sun}\\mid \\langle \\text{comes, the} \\rangle) &= \\lambda_0 \\times P_{X|H}(\\text{sun}\\mid \\langle \\rangle, \\mathcal M_0) \\\\\n",
    "&+ \\lambda_1 P_{X|H}(\\text{sun}\\mid \\langle \\text{the} \\rangle,  \\mathcal M_1) \\\\\n",
    "&+ \\lambda_2 P_{X|H}(\\text{sun}\\mid \\langle \\text{comes, the} \\rangle,  \\mathcal M_2)  \\\\\n",
    "P_{X|H}(\\text{EOS}\\mid \\langle \\text{the, sun} \\rangle) &= \\lambda_0 \\times P_{X|H}(\\text{EOS}\\mid \\langle \\rangle, \\mathcal M_0) \\\\\n",
    "&+ \\lambda_1 P_{X|H}(\\text{EOS}\\mid \\langle \\text{sun} \\rangle,  \\mathcal M_1) \\\\\n",
    "&+ \\lambda_2 P_{X|H}(\\text{EOS}\\mid \\langle \\text{the, sun} \\rangle,  \\mathcal M_2) \n",
    "\\end{align}\n",
    "\n",
    "Then the probability of the sentence under the interpolation is \n",
    "\n",
    "\\begin{align}\n",
    "P_{S}(\\langle \\text{here, comes, the, sun, EOS}\\rangle) \n",
    "&= P_{X|H}(\\text{here} \\mid \\langle \\text{BOS, BOS} \\rangle) \\\\\n",
    "&\\times P_{X|H}(\\text{comes}\\mid \\langle \\text{BOS, here} \\rangle)  \\\\\n",
    "&\\times P_{X|H}(\\text{the}\\mid \\langle \\text{here, comes} \\rangle) \\\\\n",
    "&\\times P_{X|H}(\\text{sun}\\mid \\langle \\text{comes, the} \\rangle) \\\\\n",
    "&\\times P_{X|H}(\\text{EOS}\\mid \\langle \\text{the, sun} \\rangle)\n",
    "\\end{align}\n",
    "\n",
    "Let's try and implement it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex2\" style=\"color:red\">**Exercise 2**</a> Complete the class below which implements an interpolated language model.\n",
    "\n",
    "1. Start by completing the method `get_parameter` which computes the interpolated factor $P_{X|H}$ ;\n",
    "2. then complete the method `log_prob` which should use `get_parameter` to compute the log of the interpolated probability $P_{S|N}(x_{1:m})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpolatedLM(LM):\n",
    "    \n",
    "    def __init__(self, lms, weights):\n",
    "        \"\"\"\n",
    "        This class should interpolate language models, \n",
    "            there are certain conditions that they must hold.\n",
    "            \n",
    "        :params lms: a list of language models where the lms[i] should have order i\n",
    "        :params weights: a list of positive weights that should sum to 1.0        \n",
    "        \"\"\"\n",
    "        if not lms:\n",
    "            raise ValueError('I need at least 1 language model')\n",
    "        if not all(0 < w < 1 for w in weights) and sum(weights) != 1.0:\n",
    "            raise ValueError('LM weights must sum to 1')\n",
    "        # Let's check that we have the LMs we need\n",
    "        for i, lm in enumerate(lms):\n",
    "            if lm.order() != i:\n",
    "                raise ValueError('Interpolation requires the ith LM to be of order i-1')\n",
    "        self._max_order = lms[-1].order()  # the maximum order\n",
    "        self._lms = lms\n",
    "        self._weights = weights\n",
    "        \n",
    "    def order(self):\n",
    "        return self._max_order\n",
    "                \n",
    "    def preprocess_history(self, history):\n",
    "        raise NotImplementedError('You do not need to use or implement this method')\n",
    "            \n",
    "    def cpd_items(self, history):\n",
    "        raise NotImplementedError('You do not need to use or implement this method')\n",
    "        \n",
    "    def count_ngrams(self, data_stream):\n",
    "        raise NotImplementedError('You do not need to use or implement this method')\n",
    "                    \n",
    "    def solve_mle(self):\n",
    "        raise NotImplementedError('You do not need to use or implement this method')\n",
    "                \n",
    "    def get_parameter(self, history, word):\n",
    "        \"\"\"\n",
    "        This function should return the interpolated factor P(X=w|H=h) as defined in Equation (9) above.\n",
    "    \n",
    "        :param history: a sequence of words (a tuple)\n",
    "        :param word: a word (a str)\n",
    "        :return: a float representing P(word|history) in the interpolated model\n",
    "        \"\"\"       \n",
    "        raise NotImplementedError(\"I am an assignment, implement me!\")\n",
    "    \n",
    "    def log_prob(self, sentence):\n",
    "        \"\"\"\n",
    "        Compute the log probability of a sentence under this model. \n",
    "                \n",
    "        input: \n",
    "            sentence: a sequence of tokens\n",
    "        output:\n",
    "            log probability\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"I am an assignment, implement me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train some LMs, and then interpolate them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***DEBUG***\n",
    "lms = [\n",
    "    LM(order=0, alpha=1.),        # unigram LM\n",
    "    LM(order=1, alpha=0.),     # bigram LM\n",
    "    LM(order=2, alpha=0.),  # trigram LM\n",
    "]\n",
    "# train our models\n",
    "for lm in lms:\n",
    "    lm.count_ngrams(prep_training)\n",
    "    lm.solve_mle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that the bigram LM now does improve (on training and dev data), but improving the trigram LM on novel sentences (dev data) may require carefully tuning the interpolation weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i, s in zip(range(5), prep_training):\n",
    "    rows.append([\n",
    "        'training {}'.format(i + 1), \n",
    "         InterpolatedLM(lms[0:1], [1.]).log_prob(s), \n",
    "         InterpolatedLM(lms[0:2], [1-1/10, 1/10]).log_prob(s),\n",
    "         InterpolatedLM(lms[0:3], [1-(1/5+1/10), 1/5, 1/10]).log_prob(s)\n",
    "    ])\n",
    "for i, s in zip(range(5), prep_dev):\n",
    "    rows.append([\n",
    "        'dev {}'.format(i + 1), \n",
    "         InterpolatedLM(lms[0:1], [1.]).log_prob(s), \n",
    "         InterpolatedLM(lms[0:2], [1-1/10, 1/10]).log_prob(s),\n",
    "         InterpolatedLM(lms[0:3], [1-(1/5+1/10), 1/5, 1/10]).log_prob(s)\n",
    "    ])\n",
    "print(tabulate(rows, headers=['i', 'LM1', 'LM2', 'LM3']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we start with a unigram LM alone, we then try interpolating a unigram LM and a bigram LM and improve perplexity considerably. \n",
    "\n",
    "Curiously, further interpolating trigram and fourgram LM does not really help much. This has again to do with data sparsity: our training corpus is not very large and therefore most 3-grams and 4-grams are quite rare. If there's little overlap between training and test in terms of 4-grams, the 4-gram terms in the interpolation will be mostly dominated by Laplace smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"eval\">  Evaluation\n",
    "\n",
    "The way to evaluate the performance of an LM is to test into a final application. In other words, how much the final score of the application improves. This is called *extrinsic* evaluation. Also, we can test our LM independently from an application, this is called *intrinsinc* evaluation. In this course, we are going to study the intrinsic evaluation of the LM.\n",
    "\n",
    "We generally have access to 3 datasets: \n",
    "\n",
    "* Training is used for estimating $\\boldsymbol \\theta$ (we use boldface to indicate a collection of parameters).\n",
    "* Develpment is used to make choices across models.\n",
    "* Test is used for measuring the accuracy of the model.\n",
    "   \n",
    "In n-gram LM the evaluation is defined by the **likelihood** of the model with respect to the test dataset.\n",
    "The likelihood of the parameters $\\theta$ over the test dataset is the probability that the model assigns to the dataset.\n",
    "\n",
    "We assume the test data $\\mathcal T$ consits of $K$ independent sentences each denoted $x_{1:m_k}^{(k)}$ \n",
    "\n",
    "$P(\\mathcal T| \\boldsymbol \\theta) = \\prod_{k=1}^K P_S(x_{1:m_k}^{(k)}|\\boldsymbol \\theta)$\n",
    "\n",
    "Or in form of the log-probability:\n",
    "\n",
    "$\\log P(\\mathcal T| \\theta) = \\sum_{k=1}^K \\log P_{S}(x_{1:m_k}^{(k)}| \\theta)$\n",
    "\n",
    "Then define the log-likelihood as follows:\n",
    "\n",
    "$\\mathcal L_{\\mathcal T}(\\boldsymbol \\theta) = \\sum_{k=1}^K \\log P_{S}(x_{1:m_k}^{(k)}| \\boldsymbol \\theta)$\n",
    "\n",
    "\n",
    "Then the model that assings the higher $\\mathcal L$ given the test set is the one that best fits the data. In other words, given two probabilistic models, the one that assigns a higher probability to the test data is taken as intrinsically better. One detail we need to abstract away from is differences in factorisation of the models which may cause their likelihoods not to be comparable, but for that we will define *perplexity* below. \n",
    "\n",
    "The log likelihood is used because the probability of a particular sentence according to the LM can be a very small number, and the product of these small numbers can become even smaller, and it will cause numerical\n",
    "precision problems. \n",
    "\n",
    "\n",
    "**Perplexity** of a language model on a test set is the inverse probability of the test set, normalized\n",
    "by the number of tokens. Perplexity is a notion of average branching factor, thus a LM with low perplexity can be thought of as a *less confused* LM. That is, each time it introduces a word given some history it picks from a reduced subset of the entire vocabulary (in other words, it is more certain of how to continue). \n",
    "\n",
    "If a dataset contains $t$ tokens where $t = \\sum_{k=1}^K m_k$, then the perplexity of the model given the test set is\n",
    "\n",
    "\\begin{equation}\n",
    "(6) \\qquad \\text{PP}_{\\mathcal T}(\\boldsymbol \\theta) = \\left( \\prod_{k=1}^K P_{S}(x_{1:m_k}^{(k)}|\\boldsymbol \\theta) \\right)^{-1/t}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "It's again convenient to use log and define log-perplexity\n",
    "\n",
    "\\begin{equation}\n",
    "(7) \\qquad \\log \\text{PP}_{\\mathcal T}(\\boldsymbol \\theta) = - \\frac{1}{t} \\sum_{k=1}^K \\log P_{S}(x_{1:m_k}^{(k)}|\\boldsymbol \\theta) \n",
    "\\end{equation}\n",
    "\n",
    "You can compare models in terms of the log-perplexity given you base its computation on the same test data. The lower the perplexity, the better the model is.\n",
    "\n",
    "Comparisons in terms of perplexity are only fair if the models have the same vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a name=\"ex3\" style=\"color:red\">**Exercise 3**</a> Implement the log-perplexity function below. See the function documentation for specifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_perplexity(data_stream, lm):\n",
    "    \"\"\"\n",
    "    Calculates the perplexity of the given text.\n",
    "    This is simply 2 ** cross-entropy for the text.\n",
    "    \n",
    "    This function can make use of `lm.order()`, `lm.get_parameter()`, and `lm.log_prob()` \n",
    "\n",
    "    :param data_stream: generator of sentences (each sentence is a list of words)\n",
    "    :param lm: an instance of the class LM\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"I am an assignment, implement me!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(data_stream, lm):\n",
    "    return np.exp(log_perplexity(data_stream, lm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **DEBUG**\n",
    "two_sentences_data = [\n",
    "    \"The commission did not do much .\".lower().split(),\n",
    "    \"They discussed the matter again .\".lower().split()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl = perplexity(two_sentences_data, InterpolatedLM(lms[0:1], [1]))\n",
    "print(ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity(two_sentences_data, InterpolatedLM(lms[0:2], [1-1/10, 1/10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a name=\"ex4\" style=\"color:red\">**Exercise 4**</a> Train a unigram LM, a bigram LM, and a trigram LM. Use smoothing for at least one of your models (e.g., the unigram LM). Compare them on heldout data in terms of perplexity. Make sure to include at least one interpolation model in the comparison, as shown in the table below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "rows = [\n",
    "    ('smoothed unigram', '?'),\n",
    "    ('bigram', '?'),    \n",
    "    ('trigram', '?'),\n",
    "    ('interpolate 1-2', '?'),\n",
    "    ('interpolate 1-3', '?'),\n",
    "]\n",
    "print(tabulate(rows, headers=['LM', 'heldout ppl']))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"hparams\">  Tuning hyperparameters\n",
    "\n",
    "\n",
    "In this part we are going to use NLTK in order to play with a good discounting technique for LMs.\n",
    "\n",
    "Kneser-Ney smoothing  is explained in the [textbook (Section 3.5)](https://web.stanford.edu/~jurafsky/slp3/3.pdf). Have a look at the section to get a bit of the intuition, but don't worry about the algorithmic details as we will not assess your understanding of that in this course. \n",
    "   \n",
    "\n",
    "We will be working with the package [nltk.lm](http://www.nltk.org/api/nltk.lm.html) and in particular the class `KneserNeyInterpolated` which implements the parameter estimation algorithm described in the textbook. This  technique has a hyperparameter known as the *discounting factor* which we can choose. There's no automatic way to select this factor, but we can perform a *grid search*, that is, we can choose a range of plausible values between $0$ and $1$ and try those out. With the help of a development set we can pick the LM that performs the best intrinsically, and then use that LM to score future unseen test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex5\" style=\"color:red\">**Exercise 5**</a>\n",
    "\n",
    "* Use NLTK's `KneserNeyInterpolated` class to estimate bigram and trigram LMs\n",
    "* Perform a grid search for the discout factor and use dev-set perplexity to select the best bigram LM and the best trigram LM\n",
    "* For each model, plot discount factor vs perplexity\n",
    "* Finally, use the test set to compare your best LMs in terms of perplexity\n",
    "\n",
    "**Warning** NLTK treats the $n$ in an $n$-gram language model as the *order* of that LM. This is not very starndard terminology with respect to literature on Markov models, but that's okay. Just be careful when creating language models using NLTK code. If you use our helper function `fit_and_eval` you don't need to worry about this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.lm import KneserNeyInterpolated\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from tabulate import tabulate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_eval(training, heldout, longest, discount):\n",
    "    \"\"\"\n",
    "    Train a KneserNey interpolated n-gram LM on `training` and evaluate its perplexity on `heldout`.\n",
    "    :param training: a collection of tokenized sentences (a list of list of strings)\n",
    "    :param heldout: a collection of tokenized sentences used for evaluation (list of list of strings)\n",
    "    :param longest: this is the length of the longest ngram, in other words, this is the $n$ in $n$-gram LMs (as defined in class)\n",
    "    :param discount: a number between 0 and 1 for smoothing\n",
    "    :return LM object, and model's perplexity on heldout data\n",
    "    \"\"\"\n",
    "    assert 0 <= discount <=1, \"Discount is between 0 and 1\"\n",
    "    assert longest > 0, \"Longest here is the n in n-gram LM\"\n",
    "    lm = KneserNeyInterpolated(longest, discount=discount)\n",
    "    train, vocab = padded_everygram_pipeline(longest, training)\n",
    "    lm.fit(train, vocab)\n",
    "    return lm, lm.perplexity(heldout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
