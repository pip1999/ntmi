{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Contents**\n",
    "\n",
    "* [Text Classifiers](#cls)\n",
    "    * [Generative Models](#gen)\n",
    "    * [Discriminative Models](#disc)\n",
    "    * [Logistic Regression](#lr)\n",
    "        * [Parameter Estimation](#mle)\n",
    "    * [Richer Features](#ff)\n",
    "        \n",
    "**Table of Exercises**     \n",
    "\n",
    "* [Exercise 1](#ex1) (-/1)\n",
    "* [Exercise 2](#ex2) (-/1)\n",
    "* [Exercise 3](#ex3) (-/3)\n",
    "* [Exercise 4](#ex4) (-/5)\n",
    "\n",
    "\n",
    "**General notes**\n",
    "\n",
    "* In this notebook you are expected to use $\\LaTeX$. \n",
    "* Use python3\n",
    "\n",
    "**ILOs**\n",
    "\n",
    "After completing this lab you should be able to \n",
    "\n",
    "* develop text classifiers based on logistic regression \n",
    "* estimate parameters via gradient ascent using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"cls\"> Text Classifiers\n",
    "\n",
    "\n",
    "\n",
    "Text classifiers learn to map a given piece of text to a distribution over $C$ categories. Given a dataset of labelled documents, we can use the maximum likelihood principle to estimate he parameters of this distribution.\n",
    "\n",
    "Then, we can classify unlabelled documents by predicting the mode of the distribution over classes given text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"gen\">  Generative Models\n",
    "\n",
    "A **generative model**, such as the Naive Bayes classifier, obtains the conditional distribution $Y|S=w_{1:m}$ via *probabilistic inference*. That is, it prescribes a joint distribution over text and classes assigning probability \n",
    "\n",
    "\\begin{equation}\n",
    "P_{SY}(w_{1:m}, y) = P_Y(y)P_{S|Y}(w_{1:m}|y)\n",
    "\\end{equation}\n",
    "\n",
    "to labelled data $(w_{1:m}, y)$, and uses Bayes rule to infer the posterior probability of any one class $c \\in \\mathcal Y$ given the text: \n",
    "\n",
    "\\begin{equation}\n",
    "P_{Y|S}(c|w_{1:m}) = \\frac{P_Y(c)P_{S|Y}(w_{1:m}|c)}{\\sum_{y \\in \\mathcal Y} P_Y(y)P_{S|Y}(w_{1:m}|y)}\n",
    "\\end{equation}\n",
    "\n",
    "Crucially, in order to address data sparsity while generating the complex outcome $w_{1:m}$, a generative model exploits conditional independence assumptions to factorise $P_{S|Y}(w_{1:m}|y)$ conveniently. For example, the Naive Bayes model assumes independence of words given the class.\n",
    "\n",
    "After training, we can use the model to make decisions, for example, by classifying an unlabelled document $s$ with the mode of the posterior distribution $Y|S=s$:\n",
    "\n",
    "\\begin{equation}\n",
    "y^\\star = \\mathrm{argmax}_{y \\in \\mathcal Y} ~ \\log P_Y(c) + \\log P_{S|Y}(s|c) - \\underbrace{\\log P_S(s)}_{\\text{constant}}\n",
    "\\end{equation}\n",
    "\n",
    "Generative models are elegant and, in general, they can be quite flexible. The Naive Bayes model, in particular, is not *very* flexible. It does not easily accommodate rich and highly dependent features. For example, to capture the scope of a negation or of a conjunction of contrast (e.g., `but` in `The plot was interesting , but I expected more of the acting`) we need to model long-range dependencies in the sentence. \n",
    "Writing linguistic rules for this is not imediately obvious, thus we would like to design algorithms that can eventually figure these dependencies out on their own. \n",
    "\n",
    "One way to have the NB model capture such long range dependencies is to allow it to generate complex features, beyond unigrams, for example bigrams, or skip-bigrams (pair of words that are not necessarily adjacent, for example, `(not, nice)` in `that was not very nice`). This approach has at least two caveats, such a feature space can grow very large and many features will be very sparse, and, given a class, every feature is generated from the same distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz** What is the problem of generating a very heterogenous set of features (e.g., words, bigrams, linguistic features, binary features, count features, etc.) from the same conditional distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>SOLUTION</b></summary>\n",
    "    \n",
    "They are artificially treated as if they existed in the same sample space. In reality, the sample space of words is very different from the sample space of bigrams (the latter is much bigger), for example. The frequency of these different features (which is the statistic that the feature distributions of an NBC can capture) vary widely depending on the feature type. The model has not automatic mechanism to learn the relative importance of the different feature sets.\n",
    "    \n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In T2, you designed a Naive Bayes classifier yourself. In A2, you will use scikit-learn for that, so that you can save coding time and concentrate on other aspects of the problem.\n",
    "\n",
    "Let us start by loading labelled data from nltk and preparing it in a format compatible with scikit-learn's API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import sentence_polarity  # binary sentiment classification\n",
    "from nltk.corpus import subjectivity  # binary classification\n",
    "from nltk.corpus import brown  # 15-way document classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order of difficulty: subjectivity < sentence_polarity < brown\n",
    "\n",
    "# sentence_polarity is small, so you can use it to get comfortable with the assignment\n",
    "# without having to wait too long\n",
    "\n",
    "# in a some exercises you may be asked to use a specific corpus\n",
    "\n",
    "corpus = sentence_polarity  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = corpus.categories()\n",
    "C = len(labels)\n",
    "print(\"{}-way classification:\\n{}\".format(C, '\\n'.join(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we will split our observations in three disjoint sets, 80% for training, 10% for whatever development purposes we have, and 10% for testing the generalisation of our classifier at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def prepare_corpus(nltk_corpus, categories, seed=23, BOS='<s>', EOS='</s>'):\n",
    "    \"\"\"\n",
    "    Prepare an nltk text categorization corpus in a sklearn friendly format.\n",
    "    \n",
    "    This function is very similar to what you saw in T2, but here we add BOS tokens in addition to EOS tokens \n",
    "    (while the BOS token has no effect in NBC with unigram conditionals, \n",
    "    it can be useful for some of the feature-richer classifiers we will develop here).\n",
    "    \n",
    "    :param nltk_corpus: something like sentence_polarity\n",
    "    :param categories: a list of categories (each a string), \n",
    "        sklearn will treat categories as 0-based integers, thus we will map the ith element in this list to y=i\n",
    "    :param seed: for reproducibility\n",
    "    :param BOS: if not None, start every sentence with a single BOS token\n",
    "    :param EOS: if not None, end every sentence with a single EOS token\n",
    "    :return: training, dev, test\n",
    "        each an np.array such that \n",
    "        * array[:, 0] are the inputs (documents, each a string)\n",
    "        * array[:, 1] are the outputs (labels)\n",
    "    \"\"\"\n",
    "    pairs = []    \n",
    "    prefix = [BOS] if BOS else []\n",
    "    suffix = [EOS] if EOS else []\n",
    "    for label in categories:  # here we pair doc (as a single string) and label (string)\n",
    "        # this time we will concatenate the EOS symbol to the string\n",
    "        pairs.extend((' '.join(prefix + s + suffix), label) for s in nltk_corpus.sents(categories=[label]))\n",
    "    # we turn the pairs into a numpy array\n",
    "    # np arrays are very convenient for the indexing tools np provides, as we will see\n",
    "    pairs = np.array(pairs)\n",
    "    # it's good to shuffle the pairs\n",
    "    rng = np.random.RandomState(seed)    \n",
    "    rng.shuffle(pairs)\n",
    "    # let's split the np array into training (80%), dev (10%), and test (10%)\n",
    "    num_pairs = pairs.shape[0]\n",
    "    # we can use slices to select the first 80% of the rows\n",
    "    training = pairs[0:int(num_pairs * 0.8),:]\n",
    "    # and similarly for the next 10%\n",
    "    dev = pairs[int(num_pairs * 0.8):int(num_pairs * 0.9),:]\n",
    "    # and for the last 10%\n",
    "    test = pairs[int(num_pairs * 0.9):,:] \n",
    "    return training, dev, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map your choice of corpus to sklearn's style:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, dev, test = prepare_corpus(corpus, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.shape, dev.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "print(tabulate([[dev[0, 1], dev[0, 0]], [dev[1, 1], dev[1, 0]]], headers=['label', 'doc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz** Use sklearn `sklearn.feature_extraction.text.CountVectorizer` and `sklearn.naive_bayes.MultinomialNB` to implement a Naive Bayes classifier with only a few lines of code. Use `sklearn.metrics.classification_report` to analyse performance on dev set.\n",
    "\n",
    "**Tip** You can use `sklearn.pipeline.Pipeline` to encapsulate the vectorizer and the classifier in a single object that supports the fit/predict functionality of the classifier while dealing with vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>SOLUTION</b></summary>\n",
    "    \n",
    "```python\n",
    "cls_nb = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer(ngram_range=(1,1))),  # unigram counts\n",
    "        ('clf', MultinomialNB(alpha=0.5)),             # NBC parameterised using ngram counts (equivalent to our NaiveBayesClassifier from T2)    \n",
    "    ]\n",
    ")\n",
    "cls_nb.fit(training[:, 0], training[:, 1])\n",
    "print(\"NBC {}-way\".format(C))\n",
    "print(classification_report(dev[:,1], cls_nb.predict(dev[:, 0])))    \n",
    "# Accuracy close to 0.51 for Brown and close to 0.78 for sentence_polarity\n",
    "```    \n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_nb = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer(ngram_range=(1,1))),\n",
    "        ('clf', MultinomialNB(alpha=0.5)),                 \n",
    "    ]\n",
    ")\n",
    "cls_nb.fit(training[:, 0], training[:, 1])\n",
    "print(\"NBC {}-way\".format(C))\n",
    "print(classification_report(dev[:,1], cls_nb.predict(dev[:, 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex1\" style=\"color:red\">**Exercise 1**</a> You can use CountVectorizer to obtain a heterogenous feature set by changing the `ngram_range` argument of its constructor to `(1, 2)`, for example, which gathers counts for unigrams and bigrams. \n",
    "\n",
    "* In this exercise you should use the `brown` corpus.\n",
    "* Compare two versions of NBC in terms of performance on dev set: one version should use unigram counts only, the other version should use both unigram and bigram counts.\n",
    "* Produce a table of results comparing the two classifiers in terms of precision, recall, and f1-score.\n",
    "\n",
    "\n",
    "Even though more information is presumably better, you should observe that richer features are not helping, and that's likely because of the fact that NBC cannot distinguish the different features types (unigrams vs bigrams) leading to sub-optimal use of statistics. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you know you can use sklearn to plot a confusion matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary><b>Show me how</b></summary>\n",
    "    \n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(C/2, C/2))  # create a figure that's large enough for our task\n",
    "_ = plot_confusion_matrix(\n",
    "    cls_nb, # classifier pipeline\n",
    "    dev[:,0], # inputs\n",
    "    dev[:,1], # true labels\n",
    "    xticks_rotation='vertical', # nice for visualisation\n",
    "    ax=ax  # use our large figure\n",
    ")\n",
    "plt.show()    \n",
    "```    \n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If instead of plotting, you want the actual data of the confusion matrix, you can use `sklearn.metrics.confusion_matrix`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"disc\">  Discriminative Models\n",
    "\n",
    "A **discriminative model** computes the probability $P_{Y|S}(y|w_{1:m})$ *directly* by means of a parametric function that some input text $w_{1:m}$ to a probability vector of $C$ classes. Suppose this vector-valued function is called $\\boldsymbol \\pi$, mathematically, we write:\n",
    "\n",
    "\\begin{equation}\n",
    "\\boldsymbol \\pi: \\Sigma^* \\to \\Delta_{C-1}\n",
    "\\end{equation}\n",
    "\n",
    "That is, $\\boldsymbol \\pi$ maps from the space of all strings $\\Sigma^*$ to the space of $C$-dimensional probability vectors. Then, the probability $P_{Y|S}(y|w_{1:m})$ of any pair $(w_{1:m}, y)$ is given by the $y$th element of the vector $\\boldsymbol\\pi(w_{1:m})$, that is, $\\pi_y(w_{1:m})$.\n",
    "\n",
    "As before, our models have parameters $\\boldsymbol\\theta$ that we adjust for them to perform well, in this case, the parameters control the function $\\boldsymbol\\pi$, and we write it as $\\boldsymbol\\pi(w_{1:m}; \\boldsymbol\\theta)$.\n",
    "\n",
    "**Notation guideline**\n",
    "\n",
    "From now on, we will adopt the notation that is more common (and convenient) in discriminative models. We will refer to the input text (for us, that is a sentence, a paragraph, or document) as simply $x$.  \n",
    "\n",
    "Let $X$ take on values in the space $\\Sigma^*$ of all strings, and let $Y$ take on values in the set $\\mathcal Y = \\{1, \\ldots, C\\}$, then our discriminative model prescribes the following statistical process:\n",
    "\n",
    "\\begin{equation}\n",
    "    Y|X=x \\sim \\mathrm{Cat}(\\boldsymbol\\pi(x; \\boldsymbol\\theta))\n",
    "\\end{equation}\n",
    "\n",
    "which therefore assigns probabilty $P_{Y|X}(y|x, \\boldsymbol\\theta)=\\mathrm{Cat}(y|\\boldsymbol\\pi(x; \\boldsymbol\\theta)) = \\pi_y(x; \\boldsymbol\\theta)$ to a pair $(x, y)$.\n",
    "\n",
    "\n",
    "**Terminology** See that the discriminative model *cannot* be used to generate text. It simply does not have a generative story of $x$, and thus it will always need a user (or dataset) to produce input texts. In some applications, this may be a limitation, but in *many* language technology applications, this is not at all a limitation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz** How do we classify with a discriminative model? What is the time complexity of this procedure? Assume that an evaluation of $\\boldsymbol \\pi$ takes constant time $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary><b>SOLUTION</b></summary>\n",
    "\n",
    "We output the mode of the conditional distribution:\n",
    "    \n",
    "\\begin{equation}\n",
    "    y^\\star = \\mathrm{argmax}_{y \\in \\{0, \\ldots, C\\}} ~ \\pi_y(x; \\theta)\n",
    "\\end{equation}    \n",
    "    \n",
    "This involves one assessment of $\\boldsymbol \\pi$ and then a search for the best class out of $C$ candidates, thus, with constant time for $\\boldsymbol\\pi$ itself, this takes time $\\mathcal O(t + C)$. \n",
    "    \n",
    "As we do not really get to choose $C$, its task dependent, the performance of our classifier will depend on how efficient $\\boldsymbol \\pi$ is.\n",
    "    \n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"lr\"> Logistic Regession\n",
    "\n",
    "Our first discriminative model will be a linear model, which in the context of classification (as opposed to \n",
    "regression) is better known as **logistic regression** or **log-linear model**.\n",
    "\n",
    "\n",
    "A general logistic regression model for $C$-way classification is parameterised as follows:\n",
    "\n",
    "\\begin{align}\n",
    "    Y|X=x &\\sim \\mathrm{Cat}(\\boldsymbol \\pi(x; \\theta)) \\\\\n",
    "    \\boldsymbol\\pi(x; \\theta) &= \\mathrm{softmax}(\\mathbf W \\mathbf f(x) + \\mathbf b) \\\\\n",
    "    \\boldsymbol \\theta &= \\{\\mathbf w, \\mathbf b\\}\n",
    "\\end{align}\n",
    "\n",
    "where \n",
    "\n",
    "* $\\mathbf f$ is a vector-valued function that maps the input text $x$ to a $D$-dimensional vector of real-valued features, thus $\\mathbf f(x) \\in \\mathbb R^D$ is a feature representation (or encoding) of the input text $x$;\n",
    "* $\\mathbf W  \\in \\mathbb R^{C\\times D}$ is a matrix where each of the $C$ rows is a $D$-dimensional vector of real values, each of which captures the relative contribute of a feature to one of the $C$ classes;\n",
    "* $\\mathbf b \\in \\mathbb R^C$ is a $C$-dimensional vector of real values, each of which expresses a bias towards (if positive) or against (if negative) one of the classes;\n",
    "* the linear model $\\mathbf W \\mathbf f(x) + \\mathbf b$ produces a $C$-dimensional vector of scores (or logits);\n",
    "* and the $\\mathrm{softmax}$ function maps the logits from $\\mathbb R^C$ to a $C$-dimensional probability vector in the probability simplex $\\Delta_{C-1}$.\n",
    "\n",
    "Note that we use column vectors (as commonly done in mathematics), thus when we say $\\mathbf b \\in \\mathbb R^C$ you should imagine a numpy array with shape `[C, 1]`. You should always be careful with these conventions as oftentimes software packages adopt different conventions, sometimes more or less standardised. Most technical descriptions of models (as opposed to code documentation) use column vectors, unless otherwise noted. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz** Define the softmax function, and explain why we are sure that its output can always be safely interpreted as the parameters of a Categorical distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary><b>SOLUTION</b></summary>\n",
    "\n",
    "We output the mode of the conditional distribution:\n",
    "    \n",
    "\\begin{equation}\n",
    "    \\mathrm{softmax}(\\mathbf s) = \\left\\langle \\frac{\\exp(s_1)}{\\sum_{c=1}^C \\exp(s_c)}, \\ldots, \\frac{\\exp(s_C)}{\\sum_{c=1}^C \\exp(s_c)} \\right\\rangle\n",
    "\\end{equation}    \n",
    "    \n",
    "Every element of the output is positive, because the numerator is an exponentiated number (and exp is always positive) and the denominator is positive (since it is a sum of positive quantities). Moreover, because the denominator is the sum of the numerators, we know that the output is such that its elements add up to 1, as a Categorical parameter should.\n",
    "    \n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"mle\"> Parameter estimation \n",
    "\n",
    "---\n",
    "    \n",
    "**Remark** This section is a brief review of MLE for feature-rich models, while the theory presented here has little impact on the assignment itself (as you will not be implementing parameter estimation yourself, rather you'll be delegating that to scikit-learn), your conceptual understanding of what happens in parameter estimation for these models is something we will assess in other forms (e.g., reading, exam, technical report).\n",
    "    \n",
    "---    \n",
    "    \n",
    "In MLE, we are given a dataset of observations $\\mathcal D = \\{(x^{(1)}, y^{(1)}), \\ldots, (x^{(N)}, y^{(N)})\\}$ which we use to assess the log-likelihood of any particular choice of $\\boldsymbol\\theta$:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathcal L_{\\mathcal D}(\\boldsymbol\\theta) = \\sum_{n=1}^N \\log P_{Y|X}(y^{(n)}|x^{(n)}, \\boldsymbol \\theta)\n",
    "\\end{equation}\n",
    "\n",
    "For convex functions, the MLE solution, that is, the parameter vector $\\boldsymbol \\theta$ that is such that $\\mathcal L_{\\mathcal D}(\\boldsymbol\\theta)$ attains its maximum value can be obtained by solving the following problem:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\nabla_{\\boldsymbol\\theta} \\mathcal L_{\\mathcal D}(\\boldsymbol\\theta) = \\mathbf 0\n",
    "\\end{equation}\n",
    "\n",
    "That is, by finding the $\\boldsymbol\\theta$ that makes the gradient of the log-likelihood evaluate to a vector of zeros.\n",
    "\n",
    "The *gradient* $\\nabla_{\\boldsymbol\\theta} \\mathcal L_{\\mathcal D}(\\boldsymbol\\theta)$ is the vector of partial derivatives of the log-likelihood with respect to the coordinates of the parameter vector. \n",
    "As the log-likelihood value depends on all data points, and derivatives are linear, the gradient we are looking for aggregates contributions from all observations:\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_{\\boldsymbol\\theta} \\mathcal L_{\\mathcal D}(\\boldsymbol\\theta) &= \\nabla_{\\boldsymbol\\theta} \\sum_{n=1}^N \\log P_{Y|X}(y^{(n)}|x^{(n)}, \\boldsymbol \\theta) \\\\\n",
    " &=\\sum_{n=1}^N \\nabla_{\\boldsymbol\\theta} \\log P_{Y|X}(y^{(n)}|x^{(n)}, \\boldsymbol \\theta)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "We know how to solve this equation exactly for simple models, such as the $n$-gram LM, or the NBC (it leads to the count and divide formula that we employed so many times). For some other models, we do not know how to solve this exactly, but we know how to solve it (or approximately solve it) by numerical optimisation. \n",
    "\n",
    "A general strategy that works well as long as we have a differentiable likelihood is to use gradient ascent, whereby we start with an initial guess $\\boldsymbol \\theta^{(0)}$, and iteratively refine it by taking small steps (controlled by a positive learning rate $\\eta$) in the direction of steepest ascent:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\boldsymbol \\theta^{(t)} = \\boldsymbol \\theta^{(t-1)} + \\eta \\nabla_{\\boldsymbol\\theta^{(t-1)}} \\mathcal L_{\\mathcal D}(\\boldsymbol\\theta^{(t-1)})\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "This can be rather difficult if the dataset $\\mathcal D$ is too large, because we would need to keep all data points and all predictions $\\boldsymbol \\pi(x^{(1)}; \\boldsymbol \\theta), \\ldots, \\boldsymbol \\pi(x^{(N)}; \\boldsymbol \\theta)$ in memory for every parameter update, since the gradient aggregates contributions from all of them:\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_{\\boldsymbol\\theta^{(t-1)}} \\mathcal L_{\\mathcal D}(\\boldsymbol\\theta^{(t-1)}) &= \\sum_{n=1}^N \\nabla_{\\boldsymbol\\theta} \\log P_{Y|X}(y^{(n)}|x^{(n)}, \\boldsymbol \\theta) \\\\\n",
    " &= \\sum_{n=1}^N \\nabla_{\\boldsymbol\\theta} \\log \\pi_{y^{(n)}}(x^{(n)}; \\boldsymbol\\theta)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "To circumvent this problem we use *stochastic optimisation*, whereby in each step we replace the *exact* gradient above by a *Monte Carlo* (MC) estimate of it\n",
    "\\begin{align}\n",
    "    \\nabla_{\\boldsymbol\\theta} \\mathcal L_{\\mathcal D}(\\boldsymbol\\theta) &\\overset{\\text{MC}}{\\approx} \\nabla_{\\boldsymbol\\theta} \\frac{1}{B} \\sum_{b=1}^B \\log P_{Y|X}(y^{(b)}|x^{(b)}, \\boldsymbol\\theta)\\\\\n",
    "    &\\quad\\text{where } (x^{(b)}, y^{(b)}) \\sim \\mathcal D\n",
    "\\end{align}\n",
    "which is just the sample mean of gradient vectors computed for a batch of $B$ data points sampled uniformly at random from the dataset (we can sample with or without replacement, it is equivalent in this case).\n",
    "\n",
    "\n",
    "Under some conditions about the learning rate $\\eta$ and the shape of $\\mathcal L_{\\mathcal D}(\\boldsymbol \\theta)$, this procedure is guaranteed to converge to the optimum in finite time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This procedure is seemly rather simple, but an efficient implementation requires sophisticated data structures and algorithms, since our feature functions are sparse and high-dimensional. \n",
    "\n",
    "Instead of implementing it yourself, you will work with sklearn's implementation, which is robust and correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, begin by checking the documentation `LogisticRegression?`. \n",
    "\n",
    "In sklearn, we can find a number of solvers (optimisers) that address the MLE problem above. For small datasets it's okay to use the default, but as datasets grow (e.g., brown corpus) the default can be quite slow and require too much memory. We therefore advise that you use `solver=sag`, which implements a stochastic gradient ascent algorithm for you.\n",
    "\n",
    "The other parameter that matters is the coefficient of regularisation.\n",
    "\n",
    "Linear models can have *millions* of features, and thus millions of parameters. More often than not, we have less *data* than we have parameters. This makes your function too expressive, and essentially, the optimiser can find parameter vectors that are optimal in terms of log-likelihood but terrible in terms of generalisation to heldout data.\n",
    "\n",
    "To fight this tendency to *overfit* to observations, we impose a penalty on the *complexity* of the model. You can think of the complexity of the model as the effective number of free parameters it has. These complexity penalties take the form of a cost, or *regulariser* function that judges general properties of the parameter vector, such as its norm of magnitude.\n",
    "\n",
    "One of the most common regularisers is the $L_2$ norm (length of a vector in a Euclidean space). That is the regulariser we will be using.\n",
    "\n",
    "A regularised version of MLE solves the following problem:\n",
    "\n",
    "\\begin{align}\n",
    "\\theta^\\star &= \\mathrm{argmax}_{\\boldsymbol \\theta} ~ \\mathcal L_{\\mathcal D}(\\boldsymbol \\theta) - \\lambda L_2(\\boldsymbol\\theta)\n",
    "\\end{align}\n",
    "\n",
    "where $\\lambda \\ge 0$ is the importance of the regulariser. This is a hyperparameter which we cannot fix automatically, and instead have to count of a development set, or cross-validation, to test a range of reasonable options. In sklearn, you can control this by controlling the argument `C` of `LogisticRegession`, though note that this `C` is interpreted as $\\lambda^{-1}$, thus larger $C$ means less regularisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz** Train a classifier using sklearn's LogisticRegression, for features use unigram counts via `CountVectorizer`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "    <summary><b>SOLUTION</b></summary>\n",
    "\n",
    "```python\n",
    "cls_lr = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer(ngram_range=(1,1))),  # map strings to unigram counts (as in NBC)\n",
    "        ('clf', LogisticRegression(\n",
    "            max_iter=500,  # run stochastic gradient ascent for this many iterations\n",
    "            verbose=2,     # some messages of progress\n",
    "            C=100.,        # controls regularisation\n",
    "            solver='sag')  # choice of solver ('sag' is a stochastic algorithm that is much faster and more memory efficient than the default)\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "cls_lr.fit(training[:, 0], training[:, 1])  # this make take a moment with large corpora and/or large feature sets\n",
    "print(classification_report(dev[:,1], cls_lr.predict(dev[:, 0])))    \n",
    "```\n",
    "    \n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex2\" style=\"color:red\">**Exercise 2**</a> Compare NBC and LogisticRegression.\n",
    "\n",
    "\n",
    "* In this exercise you should use the `brown` corpus.\n",
    "* Compare the two model types (NBC vs LR) as well as two types of feature spaces, namely, unigram counts only vs unigram and bigram counts.\n",
    "* You don't need to hand-tune the regularisation coefficient of LR nor NBC's smoothing, simply use `C=100.` for the former and `alpha=0.5` for the latter.\n",
    "* Produce a table of results comparing the four systems in terms of precision, recall, and f1-score on the development set.\n",
    "* Discuss your findings.\n",
    "\n",
    "**Tip** For discriminative models, the relative magnitude of the features matters more than for generative models since the goal is not to explain the input but rather the mapping to the probability of the labels, thus it is a good idea to normalise the counts. A yet more effective thing to do is to normalise the counts while also taking distributional information about the relevance of the feature into account. You don't need to code anything for that, you can simply use a *Transformer* in sklearn, to transform your counts to normalised counts (have a look at [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html?highlight=tfidf#sklearn.feature_extraction.text.TfidfTransformer), which we also use in our own solution below). A pipeline that uses this transformer looks like this:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "cls_lr = Pipeline(\n",
    "    [\n",
    "        ('vect', CountVectorizer(ngram_range=(1,1))),  # maps strings to (sparse) vectors of counts\n",
    "        ('tfidf', TfidfTransformer()),                 # maps counts to normalised tfidf scores\n",
    "        # ...\n",
    "    ]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"ff\"> Richer Features\n",
    "\n",
    "The point behind logistic regression is to expand our feature representation to include richer information about potential dependencies that signal a class or another.\n",
    "\n",
    "In this part you will develop feature functions for your logistic regression model.\n",
    "\n",
    "Our customised feature functions will fit right into the sklearn pipeline API. We provide you with the basic structure and an example, which you can modify into your favourite feature function. Check the complete example before coding anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "# base class for transforming the input in sklearn\n",
    "from sklearn.base import TransformerMixin  \n",
    "\n",
    "\n",
    "class MyFF(TransformerMixin):\n",
    "    \"\"\"\n",
    "    Our input is text (a string of space-separated tokens)\n",
    "    which we will transform to a dictionary of sparse features (using python dict).\n",
    "    \n",
    "    Check the example and then include your own ideas for features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lowercase=False, normalize_emphasis=False, unigrams=False, length=False, count_ed=False):\n",
    "        \"\"\"\n",
    "        :param lowercase: should we lowercase before doing anything?\n",
    "        :param normalize_emphasis: a toy demonstration of a rule to normalize text\n",
    "            this one will turn `soooo` into `so` and `naaah` into `nah` and keep track of how\n",
    "            many times that happened\n",
    "        :param unigrams: count unigrams\n",
    "        :param length: a length feature\n",
    "        :param count_ed: count words that end in `ed`\n",
    "        \"\"\"\n",
    "        self._lowercase = lowercase\n",
    "        self._unigrams = unigrams\n",
    "        self._length = length\n",
    "        self._count_ed = count_ed\n",
    "        self._normalize_emphrasis = normalize_emphasis\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        \"\"\"We don't need to change this\"\"\"\n",
    "        return self\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"We don't need to change this\"\"\"\n",
    "        return dict()\n",
    "\n",
    "    def _ff(self, string):\n",
    "        \"\"\"\n",
    "        This is our feature function, it maps a document to a space of real-valued features.\n",
    "        It uses python dict to represent only the features with non-zero values.\n",
    "        \n",
    "        :param string: a document as a string \n",
    "        \"returns: dict (key is string, value is int/float)\n",
    "        \"\"\"\n",
    "        fvec = Counter()  # Let's count how many times each feature fires\n",
    "        # we can do some pre-processing if we like\n",
    "        if self._lowercase:\n",
    "            string = string.lower()\n",
    "        # we then tokenize on spaces\n",
    "        s = string.split()\n",
    "        # and being applying our feature templates\n",
    "        if self._normalize_emphrasis: # here we have a toy example (use regex to find two specific patterns)\n",
    "            _s = []\n",
    "            n = 0\n",
    "            for w in s:\n",
    "                if re.match(r'so+', w.lower()):\n",
    "                    _s.append('so')\n",
    "                    n += 1\n",
    "                elif re.match(r'na+h', w.lower()):\n",
    "                    _s.append('nah')\n",
    "                    n += 1\n",
    "                else:\n",
    "                    _s.append(w)\n",
    "            s = _s\n",
    "            fvec[\"emphasis\"] = n\n",
    "        # we can count word occurrences, as this is pretty important\n",
    "        if self._unigrams:\n",
    "            fvec.update(('word={}'.format(w) for w in s))\n",
    "        # we can also keep length around\n",
    "        if self._length:\n",
    "            fvec['length'] = len(s)\n",
    "        # and count some simple patterns as well\n",
    "        if self._count_ed:\n",
    "            fvec['*ed='] = sum(1 for w in s if w.endswith('ed'))\n",
    "                   \n",
    "        return fvec\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        \"\"\"Here we transform each input (a string) into a python dict full of features\"\"\"\n",
    "        return [self._ff(s) for s in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what this does to some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = MyFF(lowercase=True, normalize_emphasis=True, unigrams=True, length=True, count_ed=True)\n",
    "ff.transform(['I loved this film !', \"Capitain Marvel is sooooo awesome !\", \"Did I like it? Naah\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how we plug it into a classifier pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "text_clf = Pipeline(\n",
    "    [\n",
    "        ('ff', MyFF(\n",
    "            unigrams=True, \n",
    "            length=True)\n",
    "        ),\n",
    "        ('dict', DictVectorizer()),   # This will convert python dicts into efficient sparse data structures\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', LogisticRegression(max_iter=500, verbose=2, C=100., solver='sag')),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning** If your feature spaces grow too large, you may run out of memory. If that happens you can replace \n",
    "`DictVectorizer` in the pipeline for `sklearn.feature_extraction.FeatureHasher`, which has a fixed memory footprint (at the expense of some accuracy, you can think of it as a lossy compression of the original features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex3\" style=\"color:red\">**Exercise 3**</a> Design feature functions and test how your logistic regression classifier reacts to them in terms of performance on development set. \n",
    "\n",
    "Note that it's easier to beat NBC where it's not already good enough (e.g., surely `brown`, maybe `sentence_polarity`) and harder to beat it where it is pretty good (e.g., subjectivity). Though note that if your feature space is too large, you may run into lack of computing resources, so be careful.  \n",
    "\n",
    "You can sample ideas from this list, from the textbook, and you can also include your own ideas.\n",
    "\n",
    "1. distributional feature other than unigram counts such as skip-bigram counts: skip-bigrams are word pairs that are not necessarily adjacent (e.g., `(I, run)` in `I like to run`). \n",
    "2. case features: help detect named-entities (e.g., Apple is likely the company, not the fruit, except when it's the first token of the sentence, in which case it's harder to tell)\n",
    "3. number, date: \n",
    "4. text normalisation: detect numbers and dates using a regular expression are replace them by tags such as NUM or DATE; stem (see `from nltk.stem.snowball import SnowballStemmer`) or lemmatize (see `from nltk.stem import WordNetLemmatizer`) words\n",
    "5. scope features: detect linguistic scope such as negation\n",
    "6. membership features: detect which words belong to a category that is relevant to the task (e.g., which or how many words in the sentence are known to generally express positive/negative sentiment, see for example `from nltk.corpus.opinion_lexicon`).\n",
    "\n",
    "**Guideline** For full points include three features, all from different categories. Please also have a look at E4 before you start your work.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex4\" style=\"color:red\">**Exercise 4**</a> Now that you have a feature function, run a complete experiment:\n",
    "\n",
    "* you should have a *baseline* which you intended to improve upon (NBC with unigram counts);\n",
    "* also pick a stronger baseline (e.g., LogisticRegression with unigram and bigram counts);\n",
    "* finally, test LogisticRegression with your feature function and see if you can beat the stronger baseline.\n",
    "* use the dev set for decisions during development;\n",
    "* for the best version of your NBC baseline, LR baseline, and your own proposed classifier, report performance on test set, including a confusion matrix;\n",
    "* discuss your findings.\n",
    "\n",
    "This exercise may be more interesting if you use `brown`, but we will accept `sentence_polarity` if your computer runs out of memory.\n",
    "\n",
    "*Tip* sometimes to find the best setting for your proposed solution you need to find good hyperparameters (for example, you may need to try a few options for `C` in logistic regression).\n",
    "\n",
    "\n",
    "**Guidelines** Three points for doing what is required including using the two features developed in E3. The final two points is for showing extra effort. This can be an extra feature or more effort put into creating an interesting feature or having to put in effort to use the full brown corpus etc. Please show/argue what extra effort you've done in the comments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
